{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [[\"The\", \"cat\", \"is\", \"cute\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bigrams = [ngrams(sent, 2) for sent in text]\n",
    "text_unigrams = [ngrams(sent, 1) for sent in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<zip at 0x1c5a95f1600>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import NgramCounter\n",
    "ngram_counts = NgramCounter(text_bigrams + text_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_counts['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Natural', 'language')\n",
      "('language', 'processing')\n",
      "('processing', 'is')\n",
      "('is', 'a')\n",
      "('a', 'field')\n",
      "('field', 'of')\n",
      "('of', 'study')\n",
      "('study', 'focused')\n",
      "('focused', 'on')\n",
      "('on', 'the')\n",
      "('the', 'interactions')\n",
      "('interactions', 'between')\n",
      "('between', 'human')\n",
      "('human', 'language')\n",
      "('language', 'and')\n",
      "('and', 'computers.')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Natural language processing is a field of study focused on the interactions between human language and computers.\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = sentence.split()\n",
    "\n",
    "# Create bigrams from the list of words\n",
    "bigrams = ngrams(words, 2)\n",
    "\n",
    "# Print the bigrams\n",
    "for bigram in bigrams:\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('This', 'is', 'a'): 2, ('is', 'a', 'repeated'): 2, ('a', 'repeated', 'sentence'): 2, ('repeated', 'sentence', '.'): 2, ('sentence', '.', 'This'): 2, ('.', 'This', 'is'): 2, ('This', 'is', 'the'): 2, ('is', 'the', 'first'): 1, ('the', 'first', 'sentence'): 1, ('first', 'sentence', '.'): 1, ('is', 'the', 'second'): 1, ('the', 'second', 'sentence'): 1, ('second', 'sentence', '.'): 1})\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "def count_ngrams(sentence1, sentence2, n):\n",
    "    # Tokenize sentences\n",
    "    words1 = word_tokenize(sentence1)\n",
    "    words2 = word_tokenize(sentence2)\n",
    "\n",
    "    # Generate n-grams\n",
    "    ngrams_sentence1 = list(ngrams(words1, n))\n",
    "    ngrams_sentence2 = list(ngrams(words2, n))\n",
    "\n",
    "    # Combine n-grams from both sentences\n",
    "    all_ngrams = ngrams_sentence1 + ngrams_sentence2\n",
    "\n",
    "    # Count occurrences\n",
    "    ngram_counts = Counter(all_ngrams)\n",
    "\n",
    "    return ngram_counts\n",
    "\n",
    "# Example usage:\n",
    "sentence1 = \"This is a repeated sentence. This is the first sentence.\"\n",
    "sentence2 = \"This is a repeated sentence. This is the second sentence.\"\n",
    "\n",
    "n = 3  # Set your desired n-gram length\n",
    "result = count_ngrams(sentence1, sentence2, n)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def find_longest_common_ngram(sentence1, sentence2):\n",
    "    # Tokenize sentences\n",
    "    words1 = word_tokenize(sentence1)\n",
    "    words2 = word_tokenize(sentence2)\n",
    "    # Find the maximum n-gram length (adjust this based on your needs)\n",
    "    max_n = min(len(words1), len(words2))\n",
    "\n",
    "    # Iterate through n-grams from max_n to 1\n",
    "    for n in range(max_n, 0, -1):\n",
    "        ngrams_sentence1 = list(ngrams(words1, n))\n",
    "        ngrams_sentence2 = list(ngrams(words2, n))\n",
    "        # Find common n-grams\n",
    "        common_ngrams = set(ngrams_sentence1) & set(ngrams_sentence2)\n",
    "        # If common n-grams are found, return the longest one\n",
    "        if common_ngrams:\n",
    "            longest_common_ngram = max(common_ngrams, key=len)\n",
    "            return len(longest_common_ngram), max_n\n",
    "    return [], max_n  # No common n-grams found\n",
    "\n",
    "sentence1 = \"This is a repeated sentence. This is the first sentence.\"\n",
    "sentence2 = \"This is a repeated sentence. This is the second sentence.\"\n",
    "result, max_n = find_longest_common_ngram(sentence1, sentence2)\n",
    "\n",
    "print(result)\n",
    "# print(len(result))\n",
    "print(max_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result = {}\n",
    "for i in range(1, 18):\n",
    "    all_result[i] = pd.read_excel(\"./excel/result.xlsx\", sheet_name=f\"{i}\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_name = 15\n",
    "number_gen = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (1, 3), (2, 3)]\n"
     ]
    }
   ],
   "source": [
    "my_list = list(range(1,number_gen+1))\n",
    "combinations = list(itertools.combinations(my_list, 2))\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_avg = []\n",
    "\n",
    "for i in combinations:\n",
    "    gold_ref = f\"gen_summary{i[0]}\"\n",
    "    gen_ref = f\"gen_summary{i[1]}\"\n",
    "    n_gram_list = []\n",
    "    # print(num)\n",
    "    for idx in range(500):\n",
    "        gold = all_result[sheet_name][gold_ref].loc[idx]\n",
    "        gen = all_result[sheet_name][gen_ref].loc[idx]\n",
    "        n_gram, max_n = find_longest_common_ngram(gold, gen)\n",
    "        n_gram_list.append(n_gram)\n",
    "        # print(len(rouge_1_list))\n",
    "    n_gram_avg.append(np.average(n_gram_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18.704, 18.216, 17.856]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_n_gram_avg = {}\n",
    "for sheet_num in range(1, 18):\n",
    "    sheet_name = sheet_num\n",
    "    n_gram_avg = []\n",
    "    n_gram_dict = {}\n",
    "    for i in combinations:\n",
    "        gold_ref = f\"gen_summary{i[0]}\"\n",
    "        gen_ref = f\"gen_summary{i[1]}\"\n",
    "        n_gram_list = []\n",
    "        # print(num)\n",
    "        for idx in range(500):\n",
    "            # print(\"=\"*100)\n",
    "            gold = all_result[sheet_name][gold_ref].loc[idx]\n",
    "            gen = all_result[sheet_name][gen_ref].loc[idx]\n",
    "            n_gram, max_n = find_longest_common_ngram(gold, gen)\n",
    "            # n_gram_list.append(n_gram/max_n)\n",
    "            n_gram_list.append(n_gram)\n",
    "            # print(idx, np.average(n_gram_list))\n",
    "        # print(len(n_gram_list))\n",
    "        # print(n_gram_list)\n",
    "        n_gram_avg.extend(n_gram_list)\n",
    "        # print(n_gram_avg)\n",
    "        n_gram_dict['n_gram'] = np.average(n_gram_avg)\n",
    "        # n_gram_dict['n_gram'] = n_gram_avg\n",
    "    all_n_gram_avg[sheet_num] = n_gram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_method = {1: '1_bart_noprompt', 2: '2_bart_topic', 3: '3_bart_length', \n",
    "               4: '4_bart_topic_length', 5: '5_bart_contrastive_random', 6: '6_bart_contrastive_synonym',\n",
    "               7: '7_bart_contrastive_combine', 8: '8_bart_contrastive_combine_word_tagger', 9: '9_bart-contrastive-combine-prompt-tagger',\n",
    "               10: '10_bart_contrastive_random_negative(5)', 11: '11_bart_contrastive_synonym_positive(6)', 12: '12_bart_contrastive_combine_pos_neg(7)',\n",
    "               13: '13_bart_topic_length_word_tagger(4)', 14: '14_bart_topic_length_word_tagger(4)',\n",
    "               15: '15_bart_topic_word_tagger', 16: '16_bart_topic_contrastive_combine', 17: '17_bart_topic_contrastive_combine_word_tagger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_bart_noprompt: 29.090666666666667\n",
      "2_bart_topic: 19.236666666666668\n",
      "3_bart_length: 17.340666666666667\n",
      "4_bart_topic_length: 13.562666666666667\n",
      "5_bart_contrastive_random: 14.036\n",
      "6_bart_contrastive_synonym: 13.644666666666666\n",
      "7_bart_contrastive_combine: 13.097333333333333\n",
      "8_bart_contrastive_combine_word_tagger: 13.098666666666666\n",
      "9_bart-contrastive-combine-prompt-tagger: 13.198\n",
      "10_bart_contrastive_random_negative(5): 14.319333333333333\n",
      "11_bart_contrastive_synonym_positive(6): 15.268\n",
      "12_bart_contrastive_combine_pos_neg(7): 13.717333333333332\n",
      "13_bart_topic_length_word_tagger(4): 13.892\n",
      "14_bart_topic_length_word_tagger(4): 13.494\n",
      "15_bart_topic_word_tagger: 18.232666666666667\n",
      "16_bart_topic_contrastive_combine: 18.847333333333335\n",
      "17_bart_topic_contrastive_combine_word_tagger: 18.258666666666667\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 18):\n",
    "    print(f\"{name_method[i]}: {all_n_gram_avg[i]['n_gram']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_bart_noprompt: 1464\n",
      "2_bart_topic: 557\n",
      "3_bart_length: 494\n",
      "4_bart_topic_length: 282\n",
      "5_bart_contrastive_random: 279\n",
      "6_bart_contrastive_synonym: 323\n",
      "7_bart_contrastive_combine: 256\n",
      "8_bart_contrastive_combine_word_tagger: 236\n",
      "9_bart-contrastive-combine-prompt-tagger: 248\n",
      "10_bart_contrastive_random_negative(5): 368\n",
      "11_bart_contrastive_synonym_positive(6): 365\n",
      "12_bart_contrastive_combine_pos_neg(7): 301\n",
      "13_bart_topic_length_word_tagger(4): 280\n",
      "14_bart_topic_length_word_tagger(4): 249\n",
      "15_bart_topic_word_tagger: 513\n",
      "16_bart_topic_contrastive_combine: 524\n",
      "17_bart_topic_contrastive_combine_word_tagger: 504\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 18):\n",
    "    print(f\"{name_method[i]}: {Counter([j for j in all_n_gram_avg[i]['n_gram'] if j == 1.0])[1.0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
